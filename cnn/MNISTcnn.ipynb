{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UH Workshop\n",
    "### CNN sample code \n",
    "(Based on: Torres and Cantu, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Larger Convolutional Neural Network for MNIST\n",
    "\n",
    "#We import classes and function then load and prepare the data the same as in the previous CNN example.\n",
    "\n",
    "# CNN for the MNIST Dataset\n",
    "import numpy\n",
    "import os\n",
    "import getopt\n",
    "import sys\n",
    "import random\n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "num_classes = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try the model using the standard MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a large CNN architecture with two convolutional layers, two max pooling layers, and two fully connected layers. The network topology can be summarized as follows:\n",
    "\n",
    "- Zero padding of size 2\n",
    "- Convolutional layer with 32 feature maps of size 4×4.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Convolutional layer with 32 feature maps of size 3×3.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Dropout layer with a probability of 20%.\n",
    "- Flatten layer.\n",
    "- Fully connected layer with 128 neurons and rectifier activation.\n",
    "- Fully connected layer with 50 neurons and rectifier activation.\n",
    "- Output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(2, 2), input_shape=(1,28,28), data_format=None))\n",
    "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the training process takes some time. We illustrate the model using only one epoch, but the optimal training with these data is with about 10 epocs. You can just change the value for epochs in model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.3246 - acc: 0.9024 - val_loss: 0.0804 - val_acc: 0.9741\n",
      "Large CNN Error: 2.59%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model weights\n",
    "model.save_weights('.../MNIST_weights')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below resizes the images to 28x28. CNN models work better with squared images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize the images to 28x28 and import them to a new directory\n",
    "\n",
    "path_from='/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/Digits/'\n",
    "path_to='/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/DigitsResized/'\n",
    "\n",
    "\n",
    "dirs_from = os.listdir( path_from )\n",
    "\n",
    "#I make sure to leave outside hidden files\n",
    "dirs_to  = []\n",
    "for item in os.listdir(path_to):\n",
    "    if not item.startswith('.'):\n",
    "        dirs_to.append(item)\n",
    "        \n",
    "\n",
    "for i in range(len(dirs_from)):\n",
    "    dirpath = path_from+dirs_from[i]\n",
    "    files = os.listdir(path_from+dirs_from[i])\n",
    "    for j in range(1,len(files)):\n",
    "        filepath = path_from+dirs_from[i]+\"/\"+files[j]\n",
    "        if \".jpg\" in filepath:\n",
    "            im = Image.open(filepath)\n",
    "            f, e = os.path.splitext(filepath)\n",
    "            imResize = im.resize((28,28), Image.ANTIALIAS)\n",
    "            filepath1= path_to+dirs_to[i]+\"/\"+files[j]\n",
    "            g, h = os.path.splitext(filepath1)\n",
    "            imResize.save(g + 'R.jpg', 'JPEG', quality=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below randomly picks the examples for the validity set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a random sample (10%) of images to the validity set\n",
    "path_from='/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/DigitsResized/'\n",
    "path_to='/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/DigitsValidation/'\n",
    "\n",
    "dirs_from  = []\n",
    "for item in os.listdir(path_from):\n",
    "    if not item.startswith('.'):\n",
    "        dirs_from.append(item)\n",
    "\n",
    "dirs_to  = []\n",
    "for item in os.listdir(path_to):\n",
    "    if not item.startswith('.'):\n",
    "        dirs_to.append(item)\n",
    "N = 100\n",
    "P = 0.10\n",
    "\n",
    "\n",
    "for i in range(len(dirs_from)):\n",
    "    dirpathO = path_from+dirs_from[i]\n",
    "    dirpathD = path_to+dirs_to[i]\n",
    "    j = 0\n",
    "    while j < (P*len(os.listdir(dirpathO))):\n",
    "        picked = random.choice(os.listdir(dirpathO))\n",
    "        fO = os.path.join(dirpathO, picked)\n",
    "        fD = os.path.join(dirpathD, picked)\n",
    "        os.rename(fO, fD)\n",
    "        j = j + 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do list: Clean this function. I am tired, sick, and lazy right now to think in loops\n",
    "#Transforming the images to 2D\n",
    "\n",
    "def get_data(folder):\n",
    "    \"\"\"\n",
    "    Load the data and labels from the given folder.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    digits=['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "    for digit in os.listdir(folder):\n",
    "        if not digit.startswith('.'):\n",
    "            for x in range(10):\n",
    "                if digit in digits[x]:\n",
    "                    label = digits[x]\n",
    "                    \n",
    "            for image_filename in os.listdir(folder + digit):\n",
    "                img_file = cv2.imread(folder + digit + '/' + image_filename, 0)\n",
    "                if img_file is not None:\n",
    "                    img_arr = numpy.asarray(img_file)\n",
    "                    X.append(img_arr)\n",
    "                    y.append(label)\n",
    "    X = numpy.asarray(X)\n",
    "    y = numpy.asarray(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A model only using our data\n",
    "\n",
    "def tuned_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(2, 2), input_shape=(1,28,28), data_format=None))\n",
    "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "#To load MNIST weights and freeze the first set of layers, uncomment the lines below:\n",
    "    #model.load_weights('/Users/franciscocantu/Dropbox/VisualAnalysis/Methods Article/Data/MNIST_weights')\n",
    "    #    model.layers\n",
    "    #for layer in model.layers[:3]:\n",
    "    #    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, y_train1 = get_data('/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/DigitsResized/')\n",
    "X_test1, y_test1 = get_data('/Users/franciscocantu/Dropbox/UH/Teaching/Workshop/Data/DigitsValidation/')\n",
    "\n",
    "nsamples, nx, ny = X_train1.shape\n",
    "nsamples2, nx2, ny2 = X_test1.shape\n",
    "X_train1 = X_train1.reshape(X_train1.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test1 = X_test1.reshape(X_test1.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train1 = X_train1 / 255\n",
    "X_test1 = X_test1 / 255\n",
    "# one hot encode outputs\n",
    "y_train1 = np_utils.to_categorical(y_train1)\n",
    "y_test1 = np_utils.to_categorical(y_test1)\n",
    "num_classes = y_test1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add to our model data augmentation, this create artifically cases from our original database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = tuned_model()\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "X_train1, y_train1 = shuffle(X_train1, y_train1)\n",
    "X_test1, y_test1 = shuffle(X_test1, y_test1)\n",
    "\n",
    "imgen_train = ImageDataGenerator( #uncomment the two lines below for data augmentation\n",
    "    zoom_range=0.15, #zoom in and out                           \n",
    "    rotation_range=20 #Degree of random rotations\n",
    ")\n",
    "\n",
    "imgen_valid = ImageDataGenerator()\n",
    "\n",
    "imgen_train.fit(X_train1)\n",
    "imgen_valid.fit(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we illustrate the model using only two epochs. You will find that the predictions of the model with these data are far from optimal. Our tests show that we should stop training this model after about 100 epochs, so don't hold your breath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5a9bacca1ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cnn = model.fit_generator(imgen_train.flow(*shuffle(X_train1, y_train1), batch_size=250), \n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 validation_data=(X_test1, y_test1))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "cnn = model.fit_generator(imgen_train.flow(*shuffle(X_train1, y_train1), batch_size=250), \n",
    "                epochs=100,\n",
    "                steps_per_epoch=250,\n",
    "                validation_data=(X_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test1, y_test1, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "Y_pred = model.predict(X_test1,verbose=2)\n",
    "y_pred = numpy.argmax(Y_pred,axis=1)\n",
    "\n",
    "# Predictions per category\n",
    "for i in range(10):\n",
    "    print(i, confusion_matrix(numpy.argmax(y_test1,axis=1), y_pred)[i].sum())\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(numpy.argmax(y_test1,axis=1), y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probabilities\n",
    "prediction_prob=pd.DataFrame(numpy.amax(Y_pred, axis=1))\n",
    "label=pd.DataFrame(numpy.argmax(y_test1,axis=1))\n",
    "y_pred=pd.DataFrame(y_pred)\n",
    "\n",
    "test = pd.concat([label, y_pred, prediction_prob], axis=1)\n",
    "test.columns = ['label','predicted','prob']\n",
    "test['flagged'] = numpy.where(test['prob']<.8, '1', '0')\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance plots\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(cnn.history['acc'], 'r')\n",
    "plt.plot(cnn.history['val_acc'], 'g')\n",
    "plt.xticks(numpy.arange(0, 25, 5.0))\n",
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(cnn.history['loss'], 'r')\n",
    "plt.plot(cnn.history['val_loss'], 'g')\n",
    "plt.xticks(numpy.arange(0, 25, 5.0))\n",
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model weights\n",
    "model.save_weights('.../Data/Tuned_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
